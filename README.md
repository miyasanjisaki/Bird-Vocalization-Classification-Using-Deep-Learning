# Bird Vocalization Classification Usage Guide

This repository contains a CNN-LSTM based bird-call detector together with a Streamlit
visualization app. The following sections explain how to run the command-line inference
script (`train/test_CNN_LSTM.py`) and the interactive web interface (`app/streamlit_app.py`).

## 1. Prepare the environment

1. Install Python 3.9+ and FFmpeg (required by Librosa for audio decoding).
2. Install the Python dependencies:

   ```bash
   pip install -r requirements.txt
   ```

   > If you do not have a `requirements.txt`, install at least `torch`, `librosa`,
   > `numpy`, `pandas`, `streamlit`, and `altair`.

3. Make sure you have:
   * The trained model weights file (default: `train/cnn_lstm_npz.pth`).
   * The preprocessed `.npz` files generated by `bird_preprocess/init.py`. They are used
     to determine the label order during inference.

You can override the default locations via environment variables:

* `BIRD_MODEL_PATH`: path to the `.pth` checkpoint.
* `BIRD_AUDIO_ROOT`: folder containing the raw audio clips you want to analyse (only for
  the CLI script when called without arguments).
* `BIRD_PROCESSED_ROOT`: directory that holds the `.npz` files created during
  preprocessing.

## 2. Command-line inference (`train/test_CNN_LSTM.py`)

The script slides a 1 s window (0.5 s hop) across each audio file, classifies each
segment, and outputs both aggregated counts per species and a detailed timeline of
predicted events.

### Basic usage

```bash
python train/test_CNN_LSTM.py <audio_folder> \
    --model-path <path/to/cnn_lstm_npz.pth> \
    --processed-root <path/to/npz_folder>
```

* `<audio_folder>` should contain the audio clips (`.wav`, `.mp3`, `.flac`).
* `--model-path` and `--processed-root` are optional if the defaults or environment
  variables already point to the correct locations.

### Outputs

* **Console summary** – total number of detected calls per species.
* **`bird_count.csv`** – table containing per-species counts plus a total row.
* **`bird_events.csv`** – detailed log where each row stores the audio file name, start
  and end time (in seconds), predicted species, and confidence score. This file lets you
  pinpoint where each call occurs in the recording.

## 3. Streamlit web app (`app/streamlit_app.py`)

The app wraps the same inference pipeline in an interactive interface. It lets you upload
an audio file, visualise detected calls on a timeline, inspect statistics, and download
the detailed CSV.

### Launch the app

```bash
streamlit run app/streamlit_app.py
```

When the app starts:

1. Adjust the model path, `.npz` directory, and optional confidence threshold in the
   **参数设置** panel if needed.
2. Upload an audio clip (`.wav`, `.mp3`, or `.flac`).
3. The app will display:
   * The audio player for quick playback.
   * A colour-coded timeline showing where each bird call occurs.
   * A statistics table summarising counts and average confidence per species.
   * A detailed events table with start/end times and confidence scores.
4. Click **下载事件明细 CSV** to export the detections for further analysis.

The model and label mapping are cached after the first load, so subsequent uploads are
faster. The interface is built with modular components (timeline chart + tables) to make
future extensions—such as additional filters or alternative visualisations—straightforward.

## 4. Troubleshooting

* If the script cannot find label `.npz` files, ensure `--processed-root` points to the
  directory where `bird_preprocess/init.py` saved them.
* If you see audio decoding errors, verify that FFmpeg is installed and accessible in
  your system path.
* GPU acceleration is used automatically when `torch.cuda.is_available()` returns
  `True`; otherwise the model runs on CPU.

Feel free to adapt the window size, hop length, or confidence threshold in the code if
needed for your dataset.
